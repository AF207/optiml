\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{amssymb}

\renewcommand{\labelitemi}{$\vartriangleright$}

\newcommand{\tr}[1]{{#1}^{T}}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\sqrnorm}[1]{{\left\lVert#1\right\rVert}^2}

\title{Numerical Optimization \\ Constrained \& Unconstrained Optimization Algorithms for Machine Learning models}

\author{Donato Meoli}

\date{\today}

\begin{document}
\maketitle

\section{Unconstrained Optimization}

\begin{algorithm}
\caption{Pseudocode for backtracking line search.}
\begin{algorithmic}
    \Procedure{\bf BLS}{$\varphi, \varphi', \alpha, m_{1}, \tau$}
        \While{($\varphi(\alpha) > \varphi(0) + m_1 \alpha \varphi'(0)$)}
            \State~$\alpha \gets \tau \, \alpha$ \Comment{$\tau < 1$}
        \EndWhile
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudocode for quadratic functions local minimum detection.}
\begin{algorithmic}
    \Procedure{\bf SDQ}{$Q, q, x, \varepsilon$}
        \While{($\norm{\nabla f(x)} > \varepsilon$)}
            \State~$d \gets - \nabla f(x)$
            \State~$\alpha \gets \sqrnorm{d} / (\tr{d} Q d$)
            \State~$x \gets x + \alpha d$
        \EndWhile
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudocode for non-quadratic functions local minimum detection.}
\begin{algorithmic}
    \Procedure{\bf SDG}{$f, x, \varepsilon$}
        \While{($\norm{\nabla f(x)} > \varepsilon$)}
            \State~$d \gets - \nabla f(x)$
            \State~$\alpha \gets$ AWLS($f(x + \alpha d)$)
            \State~$x \gets x + \alpha d$
        \EndWhile
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudocode for conjugate gradient method for quadratic functions.}
\begin{algorithmic}
    \Procedure{\bf CGQ}{$Q, q, x, \varepsilon$}
        \State~$d^- \gets 0$
            \While{($\norm{\nabla f(x)} > \varepsilon$)}
                \If{($d^- = 0$)}
                    \State~$d \gets - \nabla f(x)$
                \Else
                    \State~$\beta = (\nabla \tr{f(x)} Q d^-) / (\tr{d^-} Q d^-)$
                    \State~$d \gets - \nabla f(x) + \beta d^-$
                \EndIf
                \State~$\alpha \gets (\nabla \tr{f(x)} d) / (\tr{d} Q d)$
                \State~$x \gets x + \alpha d$
                \State~$d^- \gets d$
            \EndWhile
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudocode for conjugate gradient method for arbitrary functions. \\ We have three different formulas for $\beta$, which coincide in the quadratic case:
\begin{itemize}[leftmargin=*]
  \item Polak-RibiÃ¨re: $\beta = (\nabla \tr{f(x)} (\nabla f(x) - \nabla f(x^-))) / \sqrnorm{\nabla f(x^-)}$  
  \item Hestenes-Stiefel: $\beta = (\tr{\nabla f(x)}(\nabla f(x) - \nabla f(x^-))) / (\tr{(\nabla f(x) - \nabla f(x^-))}d^-)$
  \item Dai-Yuan: $\beta = \sqrnorm{\nabla f(x)} / (\tr{(\nabla f(x) - \nabla f(x^-))} d^-)$
\end{itemize}}
\begin{algorithmic}
    \Procedure{\bf CGA}{$f, x, \varepsilon$}
        \State~$\nabla f^- = 0$
            \While{($\norm{\nabla f(x)} > \varepsilon$)}
                \If{($\nabla f^- = 0$)}
                    \State~$d \gets - \nabla f(x)$
                \Else
                    \State~$\beta = \sqrnorm{\nabla f(x)} / \sqrnorm{\nabla f^- }$ \Comment{Fletcher-Reeves}
                    \State~$d \gets - \nabla f(x) + \beta d^-$
                \EndIf
                \State~$\alpha \gets$ AWLS($f(x + \alpha d)$)
                \State~$x^- \gets x$
                \State~$x \gets x + \alpha d$
                \State~$d^- \gets d$
                \State~$\nabla f^- \gets \nabla f(x)$
            \EndWhile
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudocode for accelerated gradient method.}
\begin{algorithmic}
    \Procedure{\bf ACCG}{$f, x, \varepsilon$}
        \State~$x^- \gets x$
        \State~$\gamma \gets 1$
        \Repeat
            \State~$\gamma^- \gets \gamma$
            \State~$\gamma \gets ( \, \sqrt{ \, 4 \gamma^2 + \gamma^4 \, } - \gamma^2 \, ) / 2$
            \State~$\beta \gets \gamma ( \, 1 \,/\, \gamma^-  \,-\, 1 \, )$
            \State~$y \gets x + \beta ( \, x \,-\, x^- \, )$
            \State~$g \gets \nabla f(y)$
            \State~$x^- \gets x$
            \State~$x \gets y - ( \, 1 \,/\, L \,) g$
        \Until{($\norm{g} > \varepsilon$)}
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudocode for boundle method.}
\begin{algorithmic}
    \Procedure{\bf PBM}{$f, g, \bar{x}, m_1, \varepsilon$}
        \State~choose $\mu$
        \State~$\mathscr{B} \gets \{(\bar{x},~f(\bar{x}),~g(\bar{x}))\}$
        \While{( \textbf{true} )}
            \State~$x^* \gets \argmin{\{f_{\mathscr{B}}(x) + \mu \sqrnorm{x - \bar{x}} / 2 \}}$
            \If{($\mu \| \, x^* - \bar{x} \, \|_2 \leq \varepsilon$)}
                \State~\textbf{break}
            \EndIf
            \If{($f(x^*) \leq f(\bar{x}) + m_1 (f_{\mathscr{B}}(x^*) - f(\bar{x}))$)}
                \State~$\bar{x} \gets x^*$
                \State~possibly decrease $\mu$
            \Else
                \State~possibly increase $\mu$
            \EndIf
            \State~$\mathscr{B} \gets \mathscr{B} \cup ( \, x^* \,,\, f(x^*) \,,\, g(x^*) \, )$
        \EndWhile
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\end{document}