import randomimport numpy as npclass MeanSquaredError:    @staticmethod    def fn(a, y):        """Return the cost associated with an output a and desired output y."""        return 0.5 * np.sum((a - y) ** 2)    @staticmethod    def delta(z, a, y):        """Return the error delta from the output layer."""        return a - yclass CrossEntropyCost:    @staticmethod    def fn(a, y):        """Return the cost associated with an output a and desired output        y. Note that np.nan_to_num is used to ensure numerical        stability. In particular, if both a and y have a 1.0        in the same slot, then the expression (1 - y) * np.log(1 - a)        returns nan. The np.nan_to_num ensures that that is converted        to the correct value (0.0).        """        return -np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))    @staticmethod    def delta(z, a, y):        """Return the error delta from the output layer. Note that the        parameter z is not used by the method. It is included in        the method's parameters in order to make the interface        consistent with the delta method for other cost classes.        """        return a - ydef sigmoid(z):    """The sigmoid function."""    return 1.0 / (1.0 + np.exp(-z))def sigmoid_prime(z):    """Derivative of the sigmoid function."""    return sigmoid(z) * (1 - sigmoid(z))class DenseLayer:    def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):        self.n_in = n_in        self.n_out = n_out        self.activation_fn = activation_fn        self.p_dropout = p_dropout        # Initialize weights and biases        self.w = np.random.normal(loc=0.0, scale=np.sqrt(1.0 / n_out), size=(n_in, n_out))        self.b = np.random.normal(loc=0.0, scale=1.0, size=(n_out,))        self.params = [self.w, self.b]    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):        self.inpt = np.array(inpt).reshape((mini_batch_size, self.n_in))        self.output = self.activation_fn((1 - self.p_dropout) * np.dot(self.inpt, self.w) + self.b)        self.y_out = np.argmax(self.output, axis=1)        self.inpt_dropout = dropout_layer(inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)        self.output_dropout = self.activation_fn(np.dot(self.inpt_dropout, self.w) + self.b)    def accuracy(self, y):        """Return the accuracy for the mini-batch."""        return np.mean(np.equal(y, self.y_out))def dropout_layer(layer, p_dropout):    mask = np.random.RandomState().binomial(n=1, p=1 - p_dropout, size=layer.shape)    return layer * maskclass Network:    def __init__(self, sizes, loss=MeanSquaredError):        """The list sizes contains the number of neurons in the respective        layers of the network. For example, if the list was [2, 3, 1]        then it would be a three-layer network, with the first layer        containing 2 neurons, the second layer 3 neurons, and the        third layer 1 neuron. The biases and weights for the network        are initialized randomly, using        self.default_weight_initializer (see docstring for that method).        """        self.num_layers = len(sizes)        self.sizes = sizes        self.default_weight_initializer()        self.loss = loss    def default_weight_initializer(self):        """Initialize each weight using a Gaussian distribution with mean 0        and standard deviation 1 over the square root of the number of        weights connecting to the same neuron. Initialize the biases        using a Gaussian distribution with mean 0 and standard        deviation 1.        Note that the first layer is assumed to be an input layer, and        by convention we won't set any biases for those neurons, since        biases are only ever used in computing the outputs from later        layers.        """        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]        self.weights = [np.random.randn(y, x) / np.sqrt(x)                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]    def large_weight_initializer(self):        """Initialize the weights using a Gaussian distribution with mean 0        and standard deviation 1. Initialize the biases using a        Gaussian distribution with mean 0 and standard deviation 1.        Note that the first layer is assumed to be an input layer, and        by convention we won't set any biases for those neurons, since        biases are only ever used in computing the outputs from later        layers.        This weight and bias initializer uses the same approach as in        Chapter 1, and is included for purposes of comparison. It        will usually be better to use the default weight initializer        instead.        """        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]        self.weights = [np.random.randn(y, x)                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]    def feedforward(self, a):        """Return the output of the network if a is input."""        for b, w in zip(self.biases, self.weights):            a = sigmoid(np.dot(w, a) + b)        return a    def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda=0.0,            evaluation_data=None,            monitor_evaluation_cost=False,            monitor_evaluation_accuracy=False,            monitor_training_cost=False,            monitor_training_accuracy=False,            early_stopping_n=0):        """Train the neural network using mini-batch stochastic gradient        descent. The training_data is a list of tuples (x, y)        representing the training inputs and the desired outputs. The        other non-optional parameters are self-explanatory, as is the        regularization parameter lmbda. The method also accepts        evaluation_data, usually either the validation or test        data. We can monitor the cost and accuracy on either the        evaluation data or the training data, by setting the        appropriate flags. The method returns a tuple containing four        lists: the (per-epoch) costs on the evaluation data, the        accuracies on the evaluation data, the costs on the training        data, and the accuracies on the training data. All values are        evaluated at the end of each training epoch. So, for example,        if we train for 30 epochs, then the first element of the tuple        will be a 30-element list containing the cost on the        evaluation data at the end of each epoch. Note that the lists        are empty if the corresponding flag is not set.        """        training_data = list(training_data)        n = len(training_data)        if evaluation_data is not None:            evaluation_data = list(evaluation_data)            n_data = len(evaluation_data)        # early stopping functionality:        best_accuracy = 0        no_accuracy_change = 0        evaluation_cost, evaluation_accuracy = [], []        training_cost, training_accuracy = [], []        for j in range(epochs):            random.shuffle(training_data)            mini_batches = [training_data[k:k + mini_batch_size]                            for k in range(0, n, mini_batch_size)]            for mini_batch in mini_batches:                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))            print("Epoch %s training complete" % j)            if monitor_training_cost:                cost = self.total_cost(training_data, lmbda)                training_cost.append(cost)                print("Cost on training data: {}".format(cost))            if monitor_training_accuracy:                accuracy = self.accuracy(training_data, convert=True)                training_accuracy.append(accuracy)                print("Accuracy on training data: {} / {}".format(accuracy, n))            if monitor_evaluation_cost:                cost = self.total_cost(evaluation_data, lmbda)                evaluation_cost.append(cost)                print("Cost on evaluation data: {}".format(cost))            if monitor_evaluation_accuracy:                accuracy = self.accuracy(evaluation_data, convert=True)                evaluation_accuracy.append(accuracy)                print("Accuracy on evaluation data: {} / {}".format(accuracy, n_data))            # Early stopping:            if early_stopping_n > 0:                if accuracy > best_accuracy:                    best_accuracy = accuracy                    no_accuracy_change = 0                    print("Early-stopping: best so far {}".format(best_accuracy))                else:                    no_accuracy_change += 1                if no_accuracy_change == early_stopping_n:                    print("Early-stopping: no accuracy change in last epochs: {}".format(early_stopping_n))                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy    def update_mini_batch(self, mini_batch, eta, lmbda, n):        """Update the network's weights and biases by applying gradient        descent using backpropagation to a single mini batch. The        mini_batch is a list of tuples (x, y), eta is the        learning rate, lmbda is the regularization parameter, and        n is the total size of the training data set.        """        nabla_b = [np.zeros(b.shape) for b in self.biases]        nabla_w = [np.zeros(w.shape) for w in self.weights]        for (x, y) in mini_batch:            delta_nabla_b, delta_nabla_w = self.back_propagation(x, y)            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw                        for w, nw in zip(self.weights, nabla_w)]        self.biases = [b - (eta / len(mini_batch)) * nb                       for b, nb in zip(self.biases, nabla_b)]    def back_propagation(self, x, y):        """Return a tuple (nabla_b, nabla_w) representing the        gradient for the cost function C_x. nabla_b and        nabla_w are layer-by-layer lists of numpy arrays, similar        to self.biases and self.weights."""        nabla_b = [np.zeros(b.shape) for b in self.biases]        nabla_w = [np.zeros(w.shape) for w in self.weights]        # feedforward        activation = x        activations = [x]  # list to store all the activations, layer by layer        zs = []  # list to store all the z vectors, layer by layer        for b, w in zip(self.biases, self.weights):            z = np.dot(w, activation) + b            zs.append(z)            activation = sigmoid(z)            activations.append(activation)        # backward pass        delta = self.loss.delta(zs[-1], activations[-1], y)        nabla_b[-1] = delta        nabla_w[-1] = np.dot(delta, activations[-2].transpose())        # Note that the variable l in the loop below is used a little        # differently to the notation in Chapter 2 of the book. Here,        # l = 1 means the last layer of neurons, l = 2 is the        # second-last layer, and so on. It's a renumbering of the        # scheme in the book, used here to take advantage of the fact        # that Python can use negative indices in lists.        for l in range(2, self.num_layers):            z = zs[-l]            sp = sigmoid_prime(z)            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp            nabla_b[-l] = delta            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())        return nabla_b, nabla_w    def accuracy(self, data, convert=False):        """Return the number of inputs in data for which the neural        network outputs the correct result. The neural network's        output is assumed to be the index of whichever neuron in the        final layer has the highest activation.        The flag convert should be set to False if the data set is        validation or test data (the usual case), and to True if the        data set is the training data. The need for this flag arises        due to differences in the way the results y are        represented in the different data sets. In particular, it        flags whether we need to convert between the different        representations. It may seem strange to use different        representations for the different data sets. Why not use the        same representation for all three data sets? It's done for        efficiency reasons -- the program usually evaluates the cost        on the training data and the accuracy on other data sets.        These are different types of computations, and using different        representations speeds things up. More details on the        representations can be found in        mnist_loader.load_data_wrapper.        """        if convert:            results = [(np.argmax(self.feedforward(x)), np.argmax(y))                       for (x, y) in data]        else:            results = [(np.argmax(self.feedforward(x)), y)                       for (x, y) in data]        result_accuracy = sum(int(x == y) for (x, y) in results)        return result_accuracy    def total_cost(self, data, lmbda, convert=False):        """Return the total cost for the data set data. The flag        convert should be set to False if the data set is the        training data (the usual case), and to True if the data set is        the validation or test data. See comments on the similar (but        reversed) convention for the accuracy method, above."""        cost = 0.0        for x, y in data:            a = self.feedforward(x)            if convert:                y = vectorized_result(y)            cost += self.loss.fn(a, y) / len(data)            cost += 0.5 * (lmbda / len(data)) * np.sum(np.sum(w ** 2) for w in self.weights)        return costdef vectorized_result(j):    """Return a 10-dimensional unit vector with a 1.0 in the j'th position    and zeroes elsewhere. This is used to convert a digit (0...9)    into a corresponding desired output from the neural network."""    e = np.zeros((3, 1))    e[j] = 1.0    return efrom abc import ABCMeta, abstractmethodimport warningsimport scipy.optimizefrom sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, is_classifierfrom sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONSfrom sklearn.neural_network._stochastic_optimizers import SGDOptimizer, AdamOptimizerfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.utils import gen_batches, check_random_state, shuffle, _safe_indexing, check_array, check_X_y, column_or_1dfrom sklearn.exceptions import ConvergenceWarningfrom sklearn.utils.extmath import safe_sparse_dotfrom sklearn.utils.validation import check_is_fittedfrom sklearn.utils.multiclass import _check_partial_fit_first_call, unique_labelsfrom sklearn.utils.multiclass import type_of_targetfrom sklearn.utils.optimize import _check_optimize_result_STOCHASTIC_SOLVERS = ['sgd', 'adam']def _pack(coefs_, intercepts_):    """Pack the parameters into a single vector."""    return np.hstack([l.ravel() for l in coefs_ + intercepts_])class BaseMultilayerPerceptron(BaseEstimator, metaclass=ABCMeta):    """Base class for MLP classification and regression.    Warning: This class should not be used directly.    Use derived classes instead.    .. versionadded:: 0.18    """    @abstractmethod    def __init__(self, hidden_layer_sizes, activation, solver,                 alpha, batch_size, learning_rate, learning_rate_init, power_t,                 max_iter, loss, shuffle, random_state, tol, verbose,                 warm_start, momentum, nesterovs_momentum, early_stopping,                 validation_fraction, beta_1, beta_2, epsilon,                 n_iter_no_change, max_fun):        self.activation = activation        self.solver = solver        self.alpha = alpha        self.batch_size = batch_size        self.learning_rate = learning_rate        self.learning_rate_init = learning_rate_init        self.power_t = power_t        self.max_iter = max_iter        self.loss = loss        self.hidden_layer_sizes = hidden_layer_sizes        self.shuffle = shuffle        self.random_state = random_state        self.tol = tol        self.verbose = verbose        self.warm_start = warm_start        self.momentum = momentum        self.nesterovs_momentum = nesterovs_momentum        self.early_stopping = early_stopping        self.validation_fraction = validation_fraction        self.beta_1 = beta_1        self.beta_2 = beta_2        self.epsilon = epsilon        self.n_iter_no_change = n_iter_no_change        self.max_fun = max_fun    def _unpack(self, packed_parameters):        """Extract the coefficients and intercepts from packed_parameters."""        for i in range(self.n_layers_ - 1):            start, end, shape = self._coef_indptr[i]            self.coefs_[i] = np.reshape(packed_parameters[start:end], shape)            start, end = self._intercept_indptr[i]            self.intercepts_[i] = packed_parameters[start:end]    def _forward_pass(self, activations):        """Perform a forward pass on the network by computing the values        of the neurons in the hidden layers and the output layer.        Parameters        ----------        activations : list, length = n_layers - 1            The ith element of the list holds the values of the ith layer.        """        hidden_activation = ACTIVATIONS[self.activation]        # Iterate over the hidden layers        for i in range(self.n_layers_ - 1):            activations[i + 1] = safe_sparse_dot(activations[i],                                                 self.coefs_[i])            activations[i + 1] += self.intercepts_[i]            # For the hidden layers            if (i + 1) != (self.n_layers_ - 1):                activations[i + 1] = hidden_activation(activations[i + 1])        # For the last layer        output_activation = ACTIVATIONS[self.out_activation_]        activations[i + 1] = output_activation(activations[i + 1])        return activations    def _compute_loss_grad(self, layer, n_samples, activations, deltas,                           coef_grads, intercept_grads):        """Compute the gradient of loss with respect to coefs and intercept for        specified layer.        This function does backpropagation for the specified one layer.        """        coef_grads[layer] = safe_sparse_dot(activations[layer].T,                                            deltas[layer])        coef_grads[layer] += (self.alpha * self.coefs_[layer])        coef_grads[layer] /= n_samples        intercept_grads[layer] = np.mean(deltas[layer], 0)        return coef_grads, intercept_grads    def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,                         coef_grads, intercept_grads):        """Compute the MLP loss function and its corresponding derivatives        with respect to the different parameters given in the initialization.        Returned gradients are packed in a single vector so it can be used        in lbfgs        Parameters        ----------        packed_coef_inter : ndarray            A vector comprising the flattened coefficients and intercepts.        X : {array-like, sparse matrix} of shape (n_samples, n_features)            The input data.        y : ndarray of shape (n_samples,)            The target values.        activations : list, length = n_layers - 1            The ith element of the list holds the values of the ith layer.        deltas : list, length = n_layers - 1            The ith element of the list holds the difference between the            activations of the i + 1 layer and the backpropagated error.            More specifically, deltas are gradients of loss with respect to z            in each layer, where z = wx + b is the value of a particular layer            before passing through the activation function        coef_grads : list, length = n_layers - 1            The ith element contains the amount of change used to update the            coefficient parameters of the ith layer in an iteration.        intercept_grads : list, length = n_layers - 1            The ith element contains the amount of change used to update the            intercept parameters of the ith layer in an iteration.        Returns        -------        loss : float        grad : array-like, shape (number of nodes of all layers,)        """        self._unpack(packed_coef_inter)        loss, coef_grads, intercept_grads = self._backprop(            X, y, activations, deltas, coef_grads, intercept_grads)        grad = _pack(coef_grads, intercept_grads)        return loss, grad    def _backprop(self, X, y, activations, deltas, coef_grads,                  intercept_grads):        """Compute the MLP loss function and its corresponding derivatives        with respect to each parameter: weights and bias vectors.        Parameters        ----------        X : {array-like, sparse matrix} of shape (n_samples, n_features)            The input data.        y : ndarray of shape (n_samples,)            The target values.        activations : list, length = n_layers - 1             The ith element of the list holds the values of the ith layer.        deltas : list, length = n_layers - 1            The ith element of the list holds the difference between the            activations of the i + 1 layer and the backpropagated error.            More specifically, deltas are gradients of loss with respect to z            in each layer, where z = wx + b is the value of a particular layer            before passing through the activation function        coef_grads : list, length = n_layers - 1            The ith element contains the amount of change used to update the            coefficient parameters of the ith layer in an iteration.        intercept_grads : list, length = n_layers - 1            The ith element contains the amount of change used to update the            intercept parameters of the ith layer in an iteration.        Returns        -------        loss : float        coef_grads : list, length = n_layers - 1        intercept_grads : list, length = n_layers - 1        """        n_samples = X.shape[0]        # Forward propagate        activations = self._forward_pass(activations)        # Get loss        loss_func_name = self.loss        if loss_func_name == 'log_loss' and self.out_activation_ == 'logistic':            loss_func_name = 'binary_log_loss'        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])        # Add L2 regularization term to loss        values = np.sum(np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))        loss += (0.5 * self.alpha) * values / n_samples        # Backward propagate        last = self.n_layers_ - 2        # The calculation of delta[last] here works with following        # combinations of output activation and loss function:        # sigmoid and binary cross entropy, softmax and categorical cross        # entropy, and identity with squared loss        deltas[last] = activations[-1] - y        # Compute gradient for the last layer        coef_grads, intercept_grads = self._compute_loss_grad(            last, n_samples, activations, deltas, coef_grads, intercept_grads)        # Iterate over the hidden layers        for i in range(self.n_layers_ - 2, 0, -1):            deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)            inplace_derivative = DERIVATIVES[self.activation]            inplace_derivative(activations[i], deltas[i - 1])            coef_grads, intercept_grads = self._compute_loss_grad(                i - 1, n_samples, activations, deltas, coef_grads,                intercept_grads)        return loss, coef_grads, intercept_grads    def _initialize(self, y, layer_units):        # set all attributes, allocate weights etc for first call        # Initialize parameters        self.n_iter_ = 0        self.t_ = 0        self.n_outputs_ = y.shape[1]        # Compute the number of layers        self.n_layers_ = len(layer_units)        # Output for regression        if not is_classifier(self):            self.out_activation_ = 'identity'        # Output for multi class        elif self._label_binarizer.y_type_ == 'multiclass':            self.out_activation_ = 'softmax'        # Output for binary class and multi-label        else:            self.out_activation_ = 'logistic'        # Initialize coefficient and intercept layers        self.coefs_ = []        self.intercepts_ = []        for i in range(self.n_layers_ - 1):            coef_init, intercept_init = self._init_coef(layer_units[i],                                                        layer_units[i + 1])            self.coefs_.append(coef_init)            self.intercepts_.append(intercept_init)        if self.solver in _STOCHASTIC_SOLVERS:            self.loss_curve_ = []            self._no_improvement_count = 0            if self.early_stopping:                self.validation_scores_ = []                self.best_validation_score_ = -np.inf            else:                self.best_loss_ = np.inf    def _init_coef(self, fan_in, fan_out):        # Use the initialization method recommended by        # Glorot et al.        factor = 6.        if self.activation == 'logistic':            factor = 2.        init_bound = np.sqrt(factor / (fan_in + fan_out))        # Generate weights and bias:        coef_init = self._random_state.uniform(-init_bound, init_bound,                                               (fan_in, fan_out))        intercept_init = self._random_state.uniform(-init_bound, init_bound,                                                    fan_out)        return coef_init, intercept_init    def _fit(self, X, y, incremental=False):        # Make sure self.hidden_layer_sizes is a list        hidden_layer_sizes = self.hidden_layer_sizes        if not hasattr(hidden_layer_sizes, "__iter__"):            hidden_layer_sizes = [hidden_layer_sizes]        hidden_layer_sizes = list(hidden_layer_sizes)        # Validate input parameters.        self._validate_hyperparameters()        if np.any(np.array(hidden_layer_sizes) <= 0):            raise ValueError("hidden_layer_sizes must be > 0, got %s." %                             hidden_layer_sizes)        X, y = self._validate_input(X, y, incremental)        n_samples, n_features = X.shape        # Ensure y is 2D        if y.ndim == 1:            y = y.reshape((-1, 1))        self.n_outputs_ = y.shape[1]        layer_units = ([n_features] + hidden_layer_sizes +                       [self.n_outputs_])        # check random state        self._random_state = check_random_state(self.random_state)        if not hasattr(self, 'coefs_') or (not self.warm_start and not        incremental):            # First time training the model            self._initialize(y, layer_units)        # lbfgs does not support mini-batches        if self.solver == 'lbfgs':            batch_size = n_samples        elif self.batch_size == 'auto':            batch_size = min(200, n_samples)        else:            if self.batch_size < 1 or self.batch_size > n_samples:                warnings.warn("Got `batch_size` less than 1 or larger than "                              "sample size. It is going to be clipped")            batch_size = np.clip(self.batch_size, 1, n_samples)        # Initialize lists        activations = [X] + [None] * (len(layer_units) - 1)        deltas = [None] * (len(activations) - 1)        coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,                                                            n_fan_out_ in zip(layer_units[:-1],                                                                              layer_units[1:])]        intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in                           layer_units[1:]]        # Run the Stochastic optimization solver        if self.solver in _STOCHASTIC_SOLVERS:            self._fit_stochastic(X, y, activations, deltas, coef_grads,                                 intercept_grads, layer_units, incremental)        # Run the LBFGS solver        elif self.solver == 'lbfgs':            self._fit_lbfgs(X, y, activations, deltas, coef_grads,                            intercept_grads, layer_units)        return self    def _validate_hyperparameters(self):        if not isinstance(self.shuffle, bool):            raise ValueError("shuffle must be either True or False, got %s." %                             self.shuffle)        if self.max_iter <= 0:            raise ValueError("max_iter must be > 0, got %s." % self.max_iter)        if self.max_fun <= 0:            raise ValueError("max_fun must be > 0, got %s." % self.max_fun)        if self.alpha < 0.0:            raise ValueError("alpha must be >= 0, got %s." % self.alpha)        if (self.learning_rate in ["constant", "invscaling", "adaptive"] and                self.learning_rate_init <= 0.0):            raise ValueError("learning_rate_init must be > 0, got %s." %                             self.learning_rate)        if self.momentum > 1 or self.momentum < 0:            raise ValueError("momentum must be >= 0 and <= 1, got %s" %                             self.momentum)        if not isinstance(self.nesterovs_momentum, bool):            raise ValueError("nesterovs_momentum must be either True or False,"                             " got %s." % self.nesterovs_momentum)        if not isinstance(self.early_stopping, bool):            raise ValueError("early_stopping must be either True or False,"                             " got %s." % self.early_stopping)        if self.validation_fraction < 0 or self.validation_fraction >= 1:            raise ValueError("validation_fraction must be >= 0 and < 1, "                             "got %s" % self.validation_fraction)        if self.beta_1 < 0 or self.beta_1 >= 1:            raise ValueError("beta_1 must be >= 0 and < 1, got %s" %                             self.beta_1)        if self.beta_2 < 0 or self.beta_2 >= 1:            raise ValueError("beta_2 must be >= 0 and < 1, got %s" %                             self.beta_2)        if self.epsilon <= 0.0:            raise ValueError("epsilon must be > 0, got %s." % self.epsilon)        if self.n_iter_no_change <= 0:            raise ValueError("n_iter_no_change must be > 0, got %s."                             % self.n_iter_no_change)        # raise ValueError if not registered        if self.activation not in ACTIVATIONS:            raise ValueError("The activation '%s' is not supported. Supported "                             "activations are %s."                             % (self.activation, list(sorted(ACTIVATIONS))))        if self.learning_rate not in ["constant", "invscaling", "adaptive"]:            raise ValueError("learning rate %s is not supported. " %                             self.learning_rate)        supported_solvers = _STOCHASTIC_SOLVERS + ["lbfgs"]        if self.solver not in supported_solvers:            raise ValueError("The solver %s is not supported. "                             " Expected one of: %s" %                             (self.solver, ", ".join(supported_solvers)))    def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,                   intercept_grads, layer_units):        # Store meta information for the parameters        self._coef_indptr = []        self._intercept_indptr = []        start = 0        # Save sizes and indices of coefficients for faster unpacking        for i in range(self.n_layers_ - 1):            n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]            end = start + (n_fan_in * n_fan_out)            self._coef_indptr.append((start, end, (n_fan_in, n_fan_out)))            start = end        # Save sizes and indices of intercepts for faster unpacking        for i in range(self.n_layers_ - 1):            end = start + layer_units[i + 1]            self._intercept_indptr.append((start, end))            start = end        # Run LBFGS        packed_coef_inter = _pack(self.coefs_,                                  self.intercepts_)        if self.verbose is True or self.verbose >= 1:            iprint = 1        else:            iprint = -1        opt_res = scipy.optimize.minimize(            self._loss_grad_lbfgs, packed_coef_inter,            method="L-BFGS-B", jac=True,            options={                "maxfun": self.max_fun,                "maxiter": self.max_iter,                "iprint": iprint,                "gtol": self.tol            },            args=(X, y, activations, deltas, coef_grads, intercept_grads))        self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)        self.loss_ = opt_res.fun        self._unpack(opt_res.x)    def _fit_stochastic(self, X, y, activations, deltas, coef_grads,                        intercept_grads, layer_units, incremental):        if not incremental or not hasattr(self, '_optimizer'):            params = self.coefs_ + self.intercepts_            if self.solver == 'sgd':                self._optimizer = SGDOptimizer(                    params, self.learning_rate_init, self.learning_rate,                    self.momentum, self.nesterovs_momentum, self.power_t)            elif self.solver == 'adam':                self._optimizer = AdamOptimizer(                    params, self.learning_rate_init, self.beta_1, self.beta_2,                    self.epsilon)        # early_stopping in partial_fit doesn't make sense        early_stopping = self.early_stopping and not incremental        if early_stopping:            # don't stratify in multilabel classification            should_stratify = is_classifier(self) and self.n_outputs_ == 1            stratify = y if should_stratify else None            X, X_val, y, y_val = train_test_split(                X, y, random_state=self._random_state,                test_size=self.validation_fraction,                stratify=stratify)            if is_classifier(self):                y_val = self._label_binarizer.inverse_transform(y_val)        else:            X_val = None            y_val = None        n_samples = X.shape[0]        sample_idx = np.arange(n_samples, dtype=int)        if self.batch_size == 'auto':            batch_size = min(200, n_samples)        else:            batch_size = np.clip(self.batch_size, 1, n_samples)        try:            for it in range(self.max_iter):                if self.shuffle:                    # Only shuffle the sample indices instead of X and y to                    # reduce the memory footprint. These indices will be used                    # to slice the X and y.                    sample_idx = shuffle(sample_idx,                                         random_state=self._random_state)                accumulated_loss = 0.0                for batch_slice in gen_batches(n_samples, batch_size):                    if self.shuffle:                        X_batch = _safe_indexing(X, sample_idx[batch_slice])                        y_batch = y[sample_idx[batch_slice]]                    else:                        X_batch = X[batch_slice]                        y_batch = y[batch_slice]                    activations[0] = X_batch                    batch_loss, coef_grads, intercept_grads = self._backprop(                        X_batch, y_batch, activations, deltas,                        coef_grads, intercept_grads)                    accumulated_loss += batch_loss * (batch_slice.stop -                                                      batch_slice.start)                    # update weights                    grads = coef_grads + intercept_grads                    self._optimizer.update_params(grads)                self.n_iter_ += 1                self.loss_ = accumulated_loss / X.shape[0]                self.t_ += n_samples                self.loss_curve_.append(self.loss_)                if self.verbose:                    print("Iteration %d, loss = %.8f" % (self.n_iter_,                                                         self.loss_))                # update no_improvement_count based on training loss or                # validation score according to early_stopping                self._update_no_improvement_count(early_stopping, X_val, y_val)                # for learning rate that needs to be updated at iteration end                self._optimizer.iteration_ends(self.t_)                if self._no_improvement_count > self.n_iter_no_change:                    # not better than last `n_iter_no_change` iterations by tol                    # stop or decrease learning rate                    if early_stopping:                        msg = ("Validation score did not improve more than "                               "tol=%f for %d consecutive epochs." % (                                   self.tol, self.n_iter_no_change))                    else:                        msg = ("Training loss did not improve more than tol=%f"                               " for %d consecutive epochs." % (                                   self.tol, self.n_iter_no_change))                    is_stopping = self._optimizer.trigger_stopping(                        msg, self.verbose)                    if is_stopping:                        break                    else:                        self._no_improvement_count = 0                if incremental:                    break                if self.n_iter_ == self.max_iter:                    warnings.warn(                        "Stochastic Optimizer: Maximum iterations (%d) "                        "reached and the optimization hasn't converged yet."                        % self.max_iter, ConvergenceWarning)        except KeyboardInterrupt:            warnings.warn("Training interrupted by user.")        if early_stopping:            # restore best weights            self.coefs_ = self._best_coefs            self.intercepts_ = self._best_intercepts    def _update_no_improvement_count(self, early_stopping, X_val, y_val):        if early_stopping:            # compute validation score, use that for stopping            self.validation_scores_.append(self.score(X_val, y_val))            if self.verbose:                print("Validation score: %f" % self.validation_scores_[-1])            # update best parameters            # use validation_scores_, not loss_curve_            # let's hope no-one overloads .score with mse            last_valid_score = self.validation_scores_[-1]            if last_valid_score < (self.best_validation_score_ +                                   self.tol):                self._no_improvement_count += 1            else:                self._no_improvement_count = 0            if last_valid_score > self.best_validation_score_:                self.best_validation_score_ = last_valid_score                self._best_coefs = [c.copy() for c in self.coefs_]                self._best_intercepts = [i.copy()                                         for i in self.intercepts_]        else:            if self.loss_curve_[-1] > self.best_loss_ - self.tol:                self._no_improvement_count += 1            else:                self._no_improvement_count = 0            if self.loss_curve_[-1] < self.best_loss_:                self.best_loss_ = self.loss_curve_[-1]    def fit(self, X, y):        """Fit the model to data matrix X and target(s) y.        Parameters        ----------        X : ndarray or sparse matrix of shape (n_samples, n_features)            The input data.        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)            The target values (class labels in classification, real numbers in            regression).        Returns        -------        self : returns a trained MLP model.        """        return self._fit(X, y, incremental=False)    @property    def partial_fit(self):        """Update the model with a single iteration over the given data.        Parameters        ----------        X : {array-like, sparse matrix} of shape (n_samples, n_features)            The input data.        y : ndarray of shape (n_samples,)            The target values.        Returns        -------        self : returns a trained MLP model.        """        if self.solver not in _STOCHASTIC_SOLVERS:            raise AttributeError("partial_fit is only available for stochastic"                                 " optimizers. %s is not stochastic."                                 % self.solver)        return self._partial_fit    def _partial_fit(self, X, y):        return self._fit(X, y, incremental=True)    def _predict(self, X):        """Predict using the trained model        Parameters        ----------        X : {array-like, sparse matrix} of shape (n_samples, n_features)            The input data.        Returns        -------        y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)            The decision function of the samples for each class in the model.        """        X = check_array(X, accept_sparse=['csr', 'csc'])        # Make sure self.hidden_layer_sizes is a list        hidden_layer_sizes = self.hidden_layer_sizes        if not hasattr(hidden_layer_sizes, "__iter__"):            hidden_layer_sizes = [hidden_layer_sizes]        hidden_layer_sizes = list(hidden_layer_sizes)        layer_units = [X.shape[1]] + hidden_layer_sizes + \                      [self.n_outputs_]        # Initialize layers        activations = [X]        for i in range(self.n_layers_ - 1):            activations.append(np.empty((X.shape[0],                                         layer_units[i + 1])))        # forward propagate        self._forward_pass(activations)        y_pred = activations[-1]        return y_predclass MLPClassifier(ClassifierMixin, BaseMultilayerPerceptron):    def __init__(self, hidden_layer_sizes=(100,), activation="relu",                 solver='adam', alpha=0.0001,                 batch_size='auto', learning_rate="constant",                 learning_rate_init=0.001, power_t=0.5, max_iter=200,                 shuffle=True, random_state=None, tol=1e-4,                 verbose=False, warm_start=False, momentum=0.9,                 nesterovs_momentum=True, early_stopping=False,                 validation_fraction=0.1, beta_1=0.9, beta_2=0.999,                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):        super().__init__(            hidden_layer_sizes=hidden_layer_sizes,            activation=activation, solver=solver, alpha=alpha,            batch_size=batch_size, learning_rate=learning_rate,            learning_rate_init=learning_rate_init, power_t=power_t,            max_iter=max_iter, loss='log_loss', shuffle=shuffle,            random_state=random_state, tol=tol, verbose=verbose,            warm_start=warm_start, momentum=momentum,            nesterovs_momentum=nesterovs_momentum,            early_stopping=early_stopping,            validation_fraction=validation_fraction,            beta_1=beta_1, beta_2=beta_2, epsilon=epsilon,            n_iter_no_change=n_iter_no_change, max_fun=max_fun)    def _validate_input(self, X, y, incremental):        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'],                         multi_output=True)        if y.ndim == 2 and y.shape[1] == 1:            y = column_or_1d(y, warn=True)        if not incremental:            self._label_binarizer = LabelBinarizer()            self._label_binarizer.fit(y)            self.classes_ = self._label_binarizer.classes_        elif self.warm_start:            classes = unique_labels(y)            if set(classes) != set(self.classes_):                raise ValueError("warm_start can only be used where `y` has "                                 "the same classes as in the previous "                                 "call to fit. Previously got %s, `y` has %s" %                                 (self.classes_, classes))        else:            classes = unique_labels(y)            if len(np.setdiff1d(classes, self.classes_, assume_unique=True)):                raise ValueError("`y` has classes not in `self.classes_`."                                 " `self.classes_` has %s. 'y' has %s." %                                 (self.classes_, classes))        y = self._label_binarizer.transform(y)        return X, y    def predict(self, X):        """Predict using the multi-layer perceptron classifier        Parameters        ----------        X : {array-like, sparse matrix} of shape (n_samples, n_features)            The input data.        Returns        -------        y : ndarray, shape (n_samples,) or (n_samples, n_classes)            The predicted classes.        """        check_is_fitted(self)        y_pred = self._predict(X)        if self.n_outputs_ == 1:            y_pred = y_pred.ravel()        return self._label_binarizer.inverse_transform(y_pred)    def fit(self, X, y):        """Fit the model to data matrix X and target(s) y.        Parameters        ----------        X : ndarray or sparse matrix of shape (n_samples, n_features)            The input data.        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)            The target values (class labels in classification, real numbers in            regression).        Returns        -------        self : returns a trained MLP model.        """        return self._fit(X, y, incremental=(self.warm_start and                                            hasattr(self, "classes_")))    @property    def partial_fit(self):        """Update the model with a single iteration over the given data.        Parameters        ----------        X : {array-like, sparse matrix}, shape (n_samples, n_features)            The input data.        y : array-like, shape (n_samples,)            The target values.        classes : array, shape (n_classes), default None            Classes across all calls to partial_fit.            Can be obtained via `np.unique(y_all)`, where y_all is the            target vector of the entire dataset.            This argument is required for the first call to partial_fit            and can be omitted in the subsequent calls.            Note that y doesn't need to contain all labels in `classes`.        Returns        -------        self : returns a trained MLP model.        """        if self.solver not in _STOCHASTIC_SOLVERS:            raise AttributeError("partial_fit is only available for stochastic"                                 " optimizer. %s is not stochastic"                                 % self.solver)        return self._partial_fit    def _partial_fit(self, X, y, classes=None):        if _check_partial_fit_first_call(self, classes):            self._label_binarizer = LabelBinarizer()            if type_of_target(y).startswith('multilabel'):                self._label_binarizer.fit(y)            else:                self._label_binarizer.fit(classes)        super()._partial_fit(X, y)        return self    def predict_log_proba(self, X):        """Return the log of probability estimates.        Parameters        ----------        X : ndarray of shape (n_samples, n_features)            The input data.        Returns        -------        log_y_prob : ndarray of shape (n_samples, n_classes)            The predicted log-probability of the sample for each class            in the model, where classes are ordered as they are in            `self.classes_`. Equivalent to log(predict_proba(X))        """        y_prob = self.predict_proba(X)        return np.log(y_prob, out=y_prob)    def predict_proba(self, X):        """Probability estimates.        Parameters        ----------        X : {array-like, sparse matrix} of shape (n_samples, n_features)            The input data.        Returns        -------        y_prob : ndarray of shape (n_samples, n_classes)            The predicted probability of the sample for each class in the            model, where classes are ordered as they are in `self.classes_`.        """        check_is_fitted(self)        y_pred = self._predict(X)        if self.n_outputs_ == 1:            y_pred = y_pred.ravel()        if y_pred.ndim == 1:            return np.vstack([1 - y_pred, y_pred]).T        else:            return y_predclass MLPRegressor(RegressorMixin, BaseMultilayerPerceptron):    def __init__(self, hidden_layer_sizes=(100,), activation="relu",                 solver='adam', alpha=0.0001,                 batch_size='auto', learning_rate="constant",                 learning_rate_init=0.001,                 power_t=0.5, max_iter=200, shuffle=True,                 random_state=None, tol=1e-4,                 verbose=False, warm_start=False, momentum=0.9,                 nesterovs_momentum=True, early_stopping=False,                 validation_fraction=0.1, beta_1=0.9, beta_2=0.999,                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):        super().__init__(            hidden_layer_sizes=hidden_layer_sizes,            activation=activation, solver=solver, alpha=alpha,            batch_size=batch_size, learning_rate=learning_rate,            learning_rate_init=learning_rate_init, power_t=power_t,            max_iter=max_iter, loss='squared_loss', shuffle=shuffle,            random_state=random_state, tol=tol, verbose=verbose,            warm_start=warm_start, momentum=momentum,            nesterovs_momentum=nesterovs_momentum,            early_stopping=early_stopping,            validation_fraction=validation_fraction,            beta_1=beta_1, beta_2=beta_2, epsilon=epsilon,            n_iter_no_change=n_iter_no_change, max_fun=max_fun)    def predict(self, X):        """Predict using the multi-layer perceptron model.        Parameters        ----------        X : {array-like, sparse matrix} of shape (n_samples, n_features)            The input data.        Returns        -------        y : ndarray of shape (n_samples, n_outputs)            The predicted values.        """        check_is_fitted(self)        y_pred = self._predict(X)        if y_pred.shape[1] == 1:            return y_pred.ravel()        return y_pred    def _validate_input(self, X, y, incremental):        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'],                         multi_output=True, y_numeric=True)        if y.ndim == 2 and y.shape[1] == 1:            y = column_or_1d(y, warn=True)        return X, yif __name__ == "__main__":    from sklearn.datasets import load_iris    X, y = load_iris(return_X_y=True)    clf = MLPClassifier(solver='adam', max_iter=1000, learning_rate_init=0.01, batch_size=10, hidden_layer_sizes=4)    clf.fit(X, y)    print(clf.predict(X))    # from ml.dataset import DataSet    #    # iris = DataSet(name='iris')    # classes = ['setosa', 'versicolor', 'virginica']    # iris.classes_to_numbers(classes)    # n_samples, n_features = len(iris.examples), iris.target    # X, y = [np.array(x[:n_features])[np.newaxis].T for x in iris.examples], \    #        [np.array(x[n_features])[np.newaxis].T for x in iris.examples]    #    # for i, t in enumerate(y):    #     y[i] = vectorized_result(t)    #    # net = Network([4, 4, 3], loss=MeanSquaredError)    # net.SGD(zip(X, y), epochs=1000, mini_batch_size=10, eta=0.01, lmbda=0.1,    #         monitor_training_accuracy=True, monitor_training_cost=True,    #         evaluation_data=zip(X, y), monitor_evaluation_accuracy=True, monitor_evaluation_cost=True)    # import ml.neural_network.mnist_loader as mnist    #    # training_data, validation_data, test_data = mnist.load_data_wrapper()    # training_data = list(training_data)    #    # # 28 x 28 pixel images = 784 input neurons    # # digits from o to 9 = 10 output neurons    # net = Network([784, 30, 10], loss=CrossEntropyCost)    # # net.large_weight_initializer()    # net.SGD(training_data, epochs=30, mini_batch_size=10, eta=0.1, lmbda=5.0,    #         monitor_training_accuracy=True, monitor_training_cost=True,    #         evaluation_data=validation_data, monitor_evaluation_accuracy=True, monitor_evaluation_cost=True)