\section{Conclusions}

For what about the SVM formulations, it is known, in general, that the \emph{primal formulation}, is suitable for large linear training since the complexity of the model grows with the number of features or, more in general, when the number of examples $n$ is much larger than the number of features $m$, i.e., $n >> m$; meanwhile the \emph{dual formulation}, is more suitable in case the number of examples $n$ is less than the number of features m, i.e., $n < m$, since the complexity of the model is dominated by the number of examples.

\bigskip

From all these experiments we can see as, for what about the \emph{primal} formulations, the results provided from the \emph{custom} implementations are strongly similar to those of \emph{sklearn} implementations, i.e., \emph{liblinear} \cite{fan2008liblinear} implementations, with a slight exception about the time gap obviously due to the different core implementation languages, Python and C respectively.

Meanwhile, for what about the \emph{dual} formulations we can notice as \emph{cvxopt} \cite{vandenberghe2010cvxopt} underperforms the \emph{sklearn} implementations, i.e., \emph{libsvm} \cite{chang2011libsvm} implementations, in terms of time since it is a general-purpose QP solver and it does not exploit the structure of the problem, as SMO does.
Despite this, the \emph{custom} implementations does not overperform the \emph{cvxopt} \cite{vandenberghe2010cvxopt} probably due to the gap generated from the different core implementation languages, again Python and C respectively.
For these reasons, \emph{sklearn} provides better results in terms of time wrt the other implementations since it is designed to work in a large-scale context and its core is implemented in C.
Furthermore, in the SVC example with the polynomial kernel of degree 5, we can see that the time gap is significatively, properly two different orders of magnitude ($\simeq$ 29min vs. $\simeq$ 19ms), and this could not depend just only by the different implementation languages; it's probable that \emph{liblinear} \cite{fan2008liblinear} adopts some heuristics, i.e., low rank approssimations of the kernel matrix, to deal with the polynomial kernel in case of high degree.

Important consideration involves the number of support vector machines: the \emph{Lagrangian dual} formulation tends to select all the data points as support vectors, so it makes the model complex and it tends to give low scores wrt the equivalent \emph{Wolfe dual} formulation. In particular, the \emph{Lagrangian relaxation} resulting from the \emph{Wolfe dual} always gives rise to a nonsmooth optimization with an exception for the SVC with a Gaussian kernel where the two formulations solve exactly the same problem. In all the other cases the goodness of the solution depends on the residue in the solution of the \emph{Lagrangian dual} at each step; one of the wrost results certainly concerns the SVC with the polynomial kernel of degree 3, where the residue is in the order of +02/03 and so the approssimation is horrible. Finally, we can see as fitting the intercept in an explicit way, i.e., by adding Lagrange multipliers to control the equality constraint, always get lower scores wrt the \emph{Lagrangian dual} of the same problem with the bias term embedded into the weight matrix.