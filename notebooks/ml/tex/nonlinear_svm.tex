\section{Nonlinear Support Vector Machines}

When applying our SVC to linearly separable data we have started by creating a matrix $Q$ from the dot product of our input variables:

\begin{equation} \label{eq:svc_hessian}
	Q_{ij}=y_i y_j 	k(x_i,x_j)
\end{equation}

or, a matrix $K$ from in the SVR case:

\begin{equation} \label{eq:svr_hessian}
	K_{ij}=k(x_i,x_j)
\end{equation}

where $k(x_i,x_j)$ is an example of a family of functions called \emph{kernel functions} and:  

\begin{equation} \label{eq:kernel_function}
	k(x_i,x_j)=\langle x_i, x_j \rangle= x_i^T x_j
\end{equation}

is known as \emph{linear} kernel.

The reason that this \emph{kernel trick} is useful is that there are many classification/regression problems that are not linearly separable/regressable in the space of the inputs $x$, which might be in a higher dimensionality feature space given a suitable mapping $x \rightarrow \phi(x)$.

\subsection{Polynomial kernel}

The \emph{polynomial} kernel is defined as:

\begin{equation} \label{eq:poly_kernel}
	k(x_i,x_j)=(\gamma \langle x_i, x_j\rangle + r)^d
\end{equation}

where $\gamma$ define how far the influence of a single training example reaches (low values meaning ‘far’ and high values meaning ‘close’).

\subsection{Gaussian kernel}

The \emph{gaussian} kernel is defined as:

\begin{equation} \label{eq:gaussian_kernel1}
	k(x_i,x_j)=\exp(-\frac{\|x_i-x_j\|^2}{2\sigma^2})
\end{equation}

or, equivalently, as:

\begin{equation} \label{eq:gaussian_kernel2}
	k(x_i,x_j)=\exp(-\gamma \|x_i-x_j\|^2)
\end{equation}

where $\displaystyle \gamma=\frac{1}{2\sigma^2}$ define how far the influence of a single training example reaches (low values meaning ‘far’ and high values meaning ‘close’).