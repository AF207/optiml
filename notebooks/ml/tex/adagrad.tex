\section{AdaGrad}

Due to the nondifferentiability of the \emph{hinge} loss, we might end up in a situation where some components of the gradient are very small and others large. So, given a learning rate, a standard gradient descent approach might end up in a situation where it decreases too quickly the small weights or too slowly the large ones.

\emph{AdaGrad} \cite{duchi2011adaptive} addresses this problem by introducing the aggregate of the squares of previously observed gradients to adjust the learning rate. This has two benefits: first, we no longer need to decide just when a gradient is large enough. Second, it scales automatically with the magnitude of the gradients. Coordinates that routinely correspond to large gradients are scaled down significantly, whereas others with small gradients receive a much more gentle treatment.

We use the variable $s_t$ to accumulate past gradient variance as follows:
	
\begin{equation} \label{eq:adagrad}
	\begin{split}
    	\begin{aligned}
        	g_t & = \partial_{{w_t}} \mathcal{L}(y_t, f(x_t, w)) \\
        	s_t & = s_{t-1} + g_t^2 \\
        	w_{t+1} & = w_t - \frac{\eta}{\sqrt{s_t + \epsilon}} \cdot g_t
    	\end{aligned}
	\end{split}
\end{equation}

\begin{algorithm}[h!]
	\caption{AdaGrad.}
	\label{alg:adagrad}
	\begin{algorithmic}[1]
		\Require{Learning rate or step size $\eta > 0$}
		\Require{Offset $\epsilon > 0$ to ensures that we do not divide by 0}
		\Procedure{AdaGrad}{}
			\State Initialize \textbf{w}
			\State $k \gets 0$
			\While {$not\_convergence$}
				\State Compute gradient estimate: $\textbf{g_k} \gets \nabla \mathcal{L}(\textbf{w_k})$
				\State Accumulate past gradient variance: $\textbf{s_k} \gets s_{k-1} + g_k^2$
				\State Apply update: $\textbf{w_{k+1}} \gets w_t - \frac{\eta}{\sqrt{s_k + \epsilon}} \cdot g_k$
				\State $k \gets k + 1$
			\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
