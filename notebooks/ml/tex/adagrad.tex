\section{AdaGrad}

Due to the nondifferentiability of the \emph{hinge} loss, we might end up in a situation where some components of the gradient are very small and others large. So, given a learning rate, a standard gradient descent approach might end up in a situation where it decreases too quickly the small weights or too slowly the large ones.

\emph{AdaGrad} \cite{duchi2011adaptive} addresses this problem by introducing the aggregate of the squares of previously observed gradients to adjust the learning rate. This has two benefits: first, we no longer need to decide just when a gradient is large enough. Second, it scales automatically with the magnitude of the gradients. Coordinates that routinely correspond to large gradients are scaled down significantly, whereas others with small gradients receive a much more gentle treatment.

\begin{algorithm}[h!]
	\caption{AdaGrad}
	\label{alg:adagrad}
	\begin{algorithmic}
		\Require{Learning rate or step size $\eta > 0$}
		\Require{Offset $\epsilon > 0$ to ensures not divide by 0}
		\Procedure{AdaGrad}{}
			\State Initialize weight vector $\textbf{w}_0$
			\State $k \gets 0$
			\While {$not\_convergence$}
				\State $\textbf{g}_k \gets \nabla \mathcal{L}(\textbf{w}_k)$
				\State $\textbf{s}_k \gets \textbf{s}_{k-1} + \textbf{g}_k^2$
				\State $\textbf{w}_{k+1} \gets \textbf{w}_k - \displaystyle \frac{\eta}{\sqrt{\textbf{s}_k + \epsilon}} \cdot \textbf{g}_k$
				\State $k \gets k + 1$
			\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
