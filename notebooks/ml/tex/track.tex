\section{Track}

(M1.1) is a \emph{Support Vector Classifier (SVC)} with the \emph{hinge} loss.

(A1.1.1) is the \emph{AdaGrad} algorithm \cite{duchi2011adaptive}, a \emph{deflected subgradient} method for solving the SVC in its \emph{primal} formulation.

(A1.1.2) is the \emph{Sequential Minimal Optimization (SMO)} algorithm \cite{platt1998sequential} (see \cite{keerthi2001improvements} for improvements), an ad hoc \emph{active set} method for training a SVC in its \emph{Wolfe dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.

(A1.1.3) is the \emph{AdaGrad} algorithm \cite{duchi2011adaptive}, a \emph{deflected subgradient} method for solving the SVC in its \emph{Lagrangian dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.

(M1.2) is a \emph{Support Vector Classifier (SVC)} with the \emph{squared hinge} loss.

(A1.2.1) is standard \emph{gradient descent} approach for solving the SVC in its \emph{primal} formulation.

(M2.1) is a \emph{Support Vector Regression (SVR)} with the \emph{epsilon-insensitive} loss.

(A2.1.1) is the \emph{AdaGrad} algorithm \cite{duchi2011adaptive}, a \emph{deflected subgradient} method for solving the SVR in its \emph{primal} formulation.

(A2.1.2) is the \emph{Sequential Minimal Optimization (SMO)} algorithm \cite{flake2002efficient} (see \cite{shevade1999improvements} for improvements), an ad hoc \emph{active set} method for training a SVR in its \emph{Wolfe dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.

(A2.1.3) is the \emph{AdaGrad} algorithm \cite{duchi2011adaptive}, a \emph{deflected subgradient} method for solving the SVR in its \emph{Lagrangian dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.

(M2.2) is a \emph{Support Vector Regression (SVR)} with the \emph{squared epsilon-insensitive} loss.

(A2.2.1) is a standard \emph{gradient descent} approach for solving the SVR in its \emph{primal} formulation.