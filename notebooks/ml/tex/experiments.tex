\section{Experiments}

The following experiments refer to 3-fold cross-validation over \emph{linearly} and \emph{nonlinearly} separable generated datasets of size 100, so the reported results are to considered as a mean over the 3 folds.

\subsection{Support Vector Classifier}

Below experiments are about the SVC for which I tested different values for the regularization hyperparameter $C$, i.e., from \emph{soft} to \emph{hard margin}, and in case of nonlinearly separable data also different \emph{kernel functions} mentioned above.

\subsubsection{Hinge loss}

\paragraph{Primal formulation}

The experiments results shown in \ref{primal_svc_hinge_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.001 and $\beta$, i.e., the \emph{momentum}, equal to 0.4. The batch size is setted to 20. Training is stopped if after 5 iterations the training loss is not lower than the best found so far.

\input{experiments/primal_svc_hinge}

\paragraph{Linear Dual formulations}

The experiments results shown in \ref{linear_lagrangian_dual_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.5 for the \emph{AdaGrad} algorithm.

\input{experiments/linear_dual_svc}

\input{experiments/linear_lagrangian_dual_svc}

\paragraph{Nonlinear Dual formulations}

The experiments results shown in \ref{nonlinear_dual_svc_cv_results} and \ref{nonlinear_lagrangian_dual_svc_cv_results} are obtained with \emph{d} and \emph{r} hyperparameters equal to 3 and 1 respectively for the \emph{polynomial} kernel; \emph{gamma} is setted to \emph{`scale`} for both \emph{polynomial} and \emph{gaussian RBF} kernels. Moreover, the experiments results shown in \ref{nonlinear_lagrangian_dual_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.5 for the \emph{AdaGrad} algorithm.

\input{experiments/nonlinear_dual_svc}

\input{experiments/nonlinear_lagrangian_dual_svc}


\subsubsection{Squared Hinge loss}

\paragraph{Primal formulation}

The experiments results shown in \ref{primal_svc_squared_hinge_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.001 and $\beta$, i.e., the \emph{momentum}, equal to 0.4. The batch size is setted to 20. Training is stopped if after 5 iterations the training loss is not lower than the best found so far.

\input{experiments/primal_svc_squared_hinge}

\pagebreak

\subsection{Support Vector Regression}

Below experiments are about the SVR for which I tested different values for regularization hyperparameter $C$, i.e., from \emph{soft} to \emph{hard margin}, the $\epsilon$ penalty value and in case of nonlinearly separable data also different \emph{kernel functions} mentioned above.

\subsubsection{Epsilon-insensitive loss}

\paragraph{Primal formulation}

The experiments results shown in \ref{primal_svr_eps_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.001 and $\beta$, i.e., the \emph{momentum}, equal to 0.4. The batch size is setted to 20. Training is stopped if after 5 iterations the training loss is not lower than the best found so far.

\input{experiments/primal_svr_eps}

\paragraph{Linear Dual formulations}

The experiments results shown in \ref{linear_lagrangian_dual_svr_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.5 for the \emph{AdaGrad} algorithm.

\input{experiments/linear_dual_svr}

\input{experiments/linear_lagrangian_dual_svr}

\paragraph{Nonlinear Dual formulations}

The experiments results shown in \ref{nonlinear_dual_svr_cv_results} and \ref{nonlinear_lagrangian_dual_svr_cv_results} are obtained with \emph{d} and \emph{r} hyperparameters both equal to 3 for the \emph{polynomial} kernel; \emph{gamma} is setted to \emph{`scale`} for both \emph{polynomial} and \emph{gaussian RBF} kernels. Moreover, the experiments results shown in \ref{nonlinear_lagrangian_dual_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.5 for the \emph{AdaGrad} algorithm.

\input{experiments/nonlinear_dual_svr}

\input{experiments/nonlinear_lagrangian_dual_svr}


\subsubsection{Squared Epsilon-insensitive loss}

\paragraph{Primal formulation}

The experiments results shown in \ref{primal_svr_squared_eps_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.001 and $\beta$, i.e., the \emph{momentum}, equal to 0.4. The batch size is setted to 20. Training is stopped if after 5 iterations the training loss is not lower than the best found so far.

\input{experiments/primal_svr_squared_eps}